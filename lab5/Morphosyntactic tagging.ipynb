{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Morphosyntactic tagging\n",
    "\n",
    "Morphosyntactic tagging is one of the core algorithms in NLP. It assigns morphological\n",
    "and (in some languages) syntactic tags to the words in a text. E.g. this allows to distinguish\n",
    "between the major grammatical categories, such as nouns and verbs.\n",
    "\n",
    "\n",
    "## Tasks\n",
    "\n",
    "1. Download [docker image](https://hub.docker.com/r/djstrong/krnnt2) of KRNNT2. It includes the following tools:\n",
    "   1. Morfeusz2 - morphological dictionary\n",
    "   1. Corpus2 - corpus access library\n",
    "   1. Toki - tokenizer for Polish\n",
    "   1. Maca - morphosyntactic analyzer\n",
    "   1. KRNNT - Polish tagger\n",
    "\n",
    ">$ docker run -it -p 9200:9200 apohllo/krnnt:0.1 python3 /home/krnnt/krnnt/krnnt_serve.py /home/krnnt/krnnt/data\n",
    "\n",
    "2. Use the tool to tag and lemmatize the law corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import regex\n",
    "\n",
    "def tag_and_lemmatize(data):\n",
    "    response = requests.post('http://localhost:9200', data=data)\n",
    "    text = response.text\n",
    "\n",
    "    reg = r\"(?<=\\n\\t)\\S*\\t\\w*\"\n",
    "    matches = regex.finditer(reg, text, regex.MULTILINE, regex.IGNORECASE)\n",
    "\n",
    "    corp = []\n",
    "    for matchNum, match in enumerate(matches, start=1):\n",
    "        spliced = match.group().split(\"\\t\")\n",
    "        combined = spliced[0].lower()+\":\"+spliced[1]\n",
    "        corp.append(combined)\n",
    "\n",
    "    return corp\n",
    "\n",
    "import os\n",
    "\n",
    "def tag_and_lemmatize_law_corpus():\n",
    "\n",
    "    # TODO: change to \"ustawy\"\n",
    "    directory = '../ustawy/'\n",
    "    fileList = os.listdir(os.getcwd() + '/' + directory)\n",
    "    corp = []\n",
    "    for filename in fileList:\n",
    "        with open(os.path.join(directory + filename), 'r') as file:\n",
    "            infile = file.read()\n",
    "            corp_ = tag_and_lemmatize(infile.encode('utf-8'))\n",
    "            corp.extend(corp_)\n",
    "            print('', end='.')\n",
    "    return corp\n",
    "\n",
    "# corp = tag_and_lemmatize_law_corpus()\n",
    "# print(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# with open(\"corpus_tag_and_lemmatized.py\", 'w') as file:\n",
    "#     file.write(\"list=\")\n",
    "#     file.write(corp.__str__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import corpus_tag_and_lemmatized\n",
    "corp = corpus_tag_and_lemmatized.list\n",
    "# print(corp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Using the tagged corpus compute bigram statistic for the tokens containing:\n",
    "   1. lemmatized, downcased word\n",
    "   > downcasowanie załatwione w poprzednim kroku\n",
    "   1. morphosyntactic **category** of the word (`subst`, `fin`, `adj`, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_bigram_statistics(corp):\n",
    "    bigrams = {}\n",
    "    for i in range(0, len(corp)-1):\n",
    "        t1 = corp[i]\n",
    "        t2 = corp[i +1]\n",
    "        if (t1, t2) not in bigrams:\n",
    "            bigrams[(t1, t2)] = 1\n",
    "        else:\n",
    "            bigrams[(t1, t2)] += 1\n",
    "\n",
    "    bigrams = dict(sorted(bigrams.items(), key= lambda a: a[1], reverse=True))\n",
    "    return bigrams\n",
    "\n",
    "bigrams = compute_bigram_statistics(corp)\n",
    "print(list(bigrams.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. Discard bigrams containing characters other than letters. Make sure that you discard the invalid entries after computing the bigram counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def discard_other_than_leters(bigrams):\n",
    "    bigrams_filtered = dict(filter(lambda a: a[0][0].split(\":\")[0].isalpha() and a[0][1].split(\":\")[0].isalpha(), bigrams.items()))\n",
    "    return bigrams_filtered\n",
    "\n",
    "bigrams_filtered = discard_other_than_leters(bigrams)\n",
    "print(list(bigrams_filtered.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "5. For example: \"Ala ma kota\", which is tagged as:\n",
    "   ```\n",
    "   Ala\tnone\n",
    "           Ala\tsubst:sg:nom:f\tdisamb\n",
    "   ma\tspace\n",
    "           mieć\tfin:sg:ter:imperf\tdisamb\n",
    "   kota\tspace\n",
    "           kot\tsubst:sg:acc:m2\tdisamb\n",
    "   .\tnone\n",
    "           .\tinterp\tdisamb\n",
    "   ```\n",
    "   the algorithm should return the following bigrams: `ala:subst mieć:fin` and `mieć:fin kot:subst`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "corp_ala = tag_and_lemmatize('Ala ma kota.'.encode('utf-8'))\n",
    "bigrams_ala = compute_bigram_statistics(corp_ala)\n",
    "bigrams_filtered_ala = discard_other_than_leters(bigrams_ala)\n",
    "print(bigrams_filtered_ala)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " >checked\n",
    "\n",
    "6. Compute LLR statistic for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# from https://github.com/tdunning/python-llr/blob/master/llr.py\n",
    "import math\n",
    "\n",
    "def denormEntropy(counts):\n",
    "    '''Computes the entropy of a list of counts scaled by the sum of the counts. If the inputs sum to one, this is just the normal definition of entropy'''\n",
    "    counts = list(counts)\n",
    "    total = float(sum(counts))\n",
    "    # Note tricky way to avoid 0*log(0)\n",
    "    return -sum([k * math.log(k/total + (k==0)) for k in counts])\n",
    "\n",
    "\n",
    "def llr_2x2(k11, k12, k21, k22):\n",
    "    '''Special case of llr with a 2x2 table'''\n",
    "    return 2 * (denormEntropy([k11+k12, k21+k22]) +\n",
    "                denormEntropy([k11+k21, k12+k22]) -\n",
    "                denormEntropy([k11, k12, k21, k22]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def calculate_freq_list():\n",
    "    freq_list= {}\n",
    "    for token in corp:\n",
    "        token = str(token).lower()\n",
    "        if token not in freq_list:\n",
    "            freq_list[str(token).lower()] =1\n",
    "        else:\n",
    "            freq_list[str(token).lower()] +=1\n",
    "\n",
    "    return dict(sorted(freq_list.items(), key= lambda a: a[1], reverse=True))\n",
    "\n",
    "freq_list = calculate_freq_list()\n",
    "\n",
    "\n",
    "def calculate_LLRs(bigrams_count, freq_list):\n",
    "    words_num = sum(list(map(lambda a: a[1], freq_list.items())))\n",
    "    bigrams_LLRs = {}\n",
    "    for item in bigrams_count.items():\n",
    "        bigram = item[0]\n",
    "        a = bigram[0]\n",
    "        b = bigram[1]\n",
    "        count = item[1]\n",
    "        a_and_b = count\n",
    "        a_without_b = freq_list[a] - a_and_b\n",
    "        b_without_a = freq_list[b] - a_and_b\n",
    "        neither_a_or_b = words_num - (freq_list[a] + freq_list[b])\n",
    "        bigrams_LLRs[bigram]=llr_2x2(a_and_b, b_without_a, a_without_b, neither_a_or_b)\n",
    "    bigrams_LLRs = dict(sorted(bigrams_LLRs.items(), key= lambda a: a[1], reverse=True))\n",
    "    return bigrams_LLRs\n",
    "\n",
    "\n",
    "bigrams_LLRs = calculate_LLRs(bigrams_filtered, freq_list)\n",
    "\n",
    "print(list(bigrams_LLRs.items())[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "7. Partition the entries based on the syntactic categories of the words, i.e. all bigrams having the form of\n",
    "   `w1:adj` `w2:subst` should be placed in one partition (the order of the words may not be changed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def partition_bigrams(bigrams_filtered):\n",
    "    partitions = {}\n",
    "    partitions_size={}\n",
    "    for big in bigrams_filtered.items():\n",
    "        cat1 = big[0][0].split(\":\")[1]\n",
    "        cat2 = big[0][1].split(\":\")[1]\n",
    "        if (cat1, cat2) not in partitions:\n",
    "            partitions[(cat1, cat2)] = {big}\n",
    "            partitions_size[(cat1, cat2)] = 1\n",
    "        else:\n",
    "            partitions[(cat1, cat2)].update({big})\n",
    "            partitions_size[(cat1, cat2)] += 1\n",
    "\n",
    "    partitions_size = dict(sorted(partitions_size.items(), key= lambda a: a[1], reverse=True))\n",
    "    return partitions, partitions_size\n",
    "\n",
    "partitions, partitions_size = partition_bigrams(bigrams_filtered)\n",
    "# print(partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "8. Select the 10 largest partitions (partitions with the larges number of entries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(list(partitions_size.items())[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "9. Use the computed LLR measure to select 5 bigrams for each of the largest categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "n = 1\n",
    "for key in partitions_size.items():\n",
    "    largest_cat = dict(partitions[key[0]])\n",
    "    print(\"Category {}: {}\".format(n, key))\n",
    "    i=0\n",
    "    for LLR_key in bigrams_LLRs.items():\n",
    "        # print(LLR_key[0])\n",
    "        if LLR_key[0] in largest_cat:\n",
    "            print(LLR_key)\n",
    "            i+=1\n",
    "        if i == 5:\n",
    "            break\n",
    "    if n == 10:\n",
    "        break\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "10. Using the results from the previous step answer the following questions:\n",
    "\n",
    "\n",
    "   i. What types of bigrams have been found?\n",
    "\n",
    "   >Znaleziono następujące typy bigramów:\n",
    ">('subst', 'adj'),('prep', 'subst'),('subst', 'subst'), ('subst', 'prep'),('adj', 'subst')\n",
    ">('conj', 'subst'),('subst', 'conj'),('subst', 'ppas'),('adj', 'conj'),('adj', 'prep')\n",
    "\n",
    "\n",
    "   ii. Which of the category-pairs indicate valuable multiword expressions?\n",
    "\n",
    "   >W mojej opinii najbardziej wartościowymi grupami wyrażeń wielowyrazowych są:\n",
    ">(subst, adj)(subst, subst)(adj, subst).\n",
    "\n",
    "    Do they have anything in common?\n",
    "\n",
    ">W każdym z nich wstępuje (subst) rzeczownik.\n",
    "\n",
    "\n",
    "   iii. Which signal: LLR score or syntactic category is more useful for determining genuine multiword expressions?\n",
    "\n",
    "   >Jednostki wielowyrazowe najlepiej typować przy wykorzystaniu podejścia hybrydowego.\n",
    ">LLR mierzy nie tylko kolokacyjność ale także częstość występowania.\n",
    "\n",
    "\n",
    "   iiii. Can you describe a different use-case where the morphosyntactic category is useful for resolving a real-world\n",
    "      problem?\n",
    "\n",
    "   > Rozpoznawanie kategorii morfosyntaktycznych może być szczególnie przydatne w tłumaczeniu długich fraz.\n",
    ">Długie sentencje zawierają często frazy oddzielone przecinkami.\n",
    ">Szczegółowa analiza relacji między poszczególnymi frazami może poprawić jakość parsowania, która będzie przyczyniać się do wzrostu jakości tłumaczenia.\n",
    "\n",
    "\n",
    "\n",
    "## Hints\n",
    "\n",
    "1. A morphosyntactic analyzer provides the possible values of morphosyntactic tags for the words.\n",
    "   E.g. for Polish \"ma\" word it can produce the following interpretations:\n",
    "   ```\n",
    "    ma\tspace\n",
    "            mieć\tfin:sg:ter:imperf\n",
    "            mój  \tadj:sg:nom:f:pos\n",
    "            mój  \tadj:sg:voc:f:pos\n",
    "   ```\n",
    "   1. The first interpretation shows that the word can be a verb in singular, in 3rd person.\n",
    "   1. The second interpretation shows that the word can be an adjective in singular, in nominative, in feminine.\n",
    "   1. The third interpretation shows that the word can be an adjective in singular, in vocative, in feminine.\n",
    "1. The full list of tags is available at [NKJP](http://nkjp.pl/poliqarp/help/ense2.html).\n",
    "1. A morphosyntactic tagger selects one of the interpretation of a word, taking into account its context.\n",
    "   It can take the interpretation from a dictionary (like KRNNT), but it can also compute it dynamically (e.g.\n",
    "   [COMBO](https://github.com/360er0/COMBO) is a tagger that does not need a morphosyntactic analyzer).\n",
    "1. The information provided by a tagger can be useful for many applications. You can selects words from particular\n",
    "   grammatical category or you can submit the data to a downstream task such as text classification.\n",
    "1. More sophisticated algorithms for multiword expressions identification, such as\n",
    "   [AutoPhrase](https://github.com/shangjingbo1226/AutoPhrase) take into account more features including:\n",
    "   morphosyntactic tags, expression contexts, etc. and use data from e.g. Wikipedia, to automatically identify\n",
    "   high-quality multiword expressions and use them to train MWE classifiers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}